{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d5a4c8-0162-4986-9b6f-b6acc967c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI Land Cover Classification Pipeline\n",
      "==================================================\n",
      "Loading training data...\n",
      "Training data shape: (8000, 30)\n",
      "Classes: class\n",
      "forest        6159\n",
      "farm           841\n",
      "impervious     669\n",
      "grass          196\n",
      "water          105\n",
      "orchard         30\n",
      "Name: count, dtype: int64\n",
      "Starting preprocessing...\n",
      "Found 27 NDVI time points\n",
      "Converting NDVI columns to numeric...\n",
      "Processing NDVI data...\n",
      "Rescaling NDVI values to standard range...\n",
      "Extracting features...\n",
      "Generated 27 features\n",
      "Training features shape: (8000, 27)\n",
      "Training Random Forest model...\n",
      "Out-of-bag score: 0.8769\n",
      "\n",
      "Classification Report (on training data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        farm       0.65      0.93      0.77       841\n",
      "      forest       0.99      0.92      0.95      6159\n",
      "       grass       0.71      0.97      0.82       196\n",
      "  impervious       0.91      0.91      0.91       669\n",
      "     orchard       0.56      1.00      0.71        30\n",
      "       water       0.94      0.96      0.95       105\n",
      "\n",
      "    accuracy                           0.92      8000\n",
      "   macro avg       0.79      0.95      0.85      8000\n",
      "weighted avg       0.94      0.92      0.93      8000\n",
      "\n",
      "\n",
      "Predicted class distribution (training):\n",
      "forest        5706\n",
      "farm          1195\n",
      "impervious     669\n",
      "grass          268\n",
      "water          108\n",
      "orchard         54\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                feature  importance\n",
      "2              ndvi_min    0.112156\n",
      "1              ndvi_std    0.082980\n",
      "24              ndvi_t2    0.072701\n",
      "0             ndvi_mean    0.051791\n",
      "4            ndvi_range    0.046295\n",
      "18          ndvi_q4_max    0.045458\n",
      "17         ndvi_q4_mean    0.044631\n",
      "22              ndvi_t0    0.041110\n",
      "9    ndvi_below_0_count    0.040823\n",
      "7   ndvi_above_04_count    0.038160\n",
      "3              ndvi_max    0.036722\n",
      "14          ndvi_q2_max    0.033781\n",
      "26              ndvi_t4    0.033575\n",
      "8   ndvi_above_06_count    0.032828\n",
      "5           ndvi_median    0.031642\n",
      "\n",
      "Loading test data...\n",
      "Test data shape: (2845, 29)\n",
      "Starting preprocessing...\n",
      "Found 27 NDVI time points\n",
      "Converting NDVI columns to numeric...\n",
      "Processing NDVI data...\n",
      "Rescaling NDVI values to standard range...\n",
      "Extracting features...\n",
      "Generated 27 features\n",
      "Making predictions...\n",
      "\n",
      "Submission saved to 'submission.csv'\n",
      "Predictions distribution:\n",
      "class\n",
      "forest        1453\n",
      "farm           633\n",
      "impervious     407\n",
      "grass          214\n",
      "water          118\n",
      "orchard         20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample predictions:\n",
      "   ID   class\n",
      "0   1  forest\n",
      "1   2  forest\n",
      "2   3  forest\n",
      "3   4    farm\n",
      "4   5  forest\n",
      "5   6    farm\n",
      "6   7    farm\n",
      "7   8  forest\n",
      "8   9    farm\n",
      "9  10    farm\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Check your directory for 'submission.csv' file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class NDVILandCoverClassifier:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # Use RandomForest but with more conservative parameters\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,           # Fewer trees to reduce overfitting\n",
    "            max_depth=10,               # More controlled depth\n",
    "            min_samples_split=10,       # Require more samples for splits\n",
    "            min_samples_leaf=4,         # Require more samples in leaf nodes\n",
    "            max_features='sqrt',        # Use sqrt(n_features) for each split decision\n",
    "            bootstrap=True,\n",
    "            oob_score=True,             # Out-of-bag scoring for monitoring\n",
    "            class_weight='balanced',    # Balance class weights\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1                   # Use all available cores\n",
    "        )\n",
    "        self.ndvi_columns = []\n",
    "        \n",
    "    def extract_ndvi_columns(self, df):\n",
    "        \"\"\"Extract NDVI time series columns\"\"\"\n",
    "        ndvi_cols = [col for col in df.columns if col.endswith('_N')]\n",
    "        ndvi_cols.sort()\n",
    "        return ndvi_cols\n",
    "    \n",
    "    def clean_data(self, df, ndvi_cols):\n",
    "        \"\"\"Clean NDVI data\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        print(\"Converting NDVI columns to numeric...\")\n",
    "        for col in ndvi_cols:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def preprocess_ndvi(self, df, ndvi_cols):\n",
    "        \"\"\"Basic preprocessing of NDVI values\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        print(\"Processing NDVI data...\")\n",
    "        \n",
    "        # Simple median imputation for missing values\n",
    "        for col in ndvi_cols:\n",
    "            median = df_processed[col].median()\n",
    "            if np.isnan(median):\n",
    "                median = 0.3  # Reasonable NDVI value\n",
    "            df_processed[col].fillna(median, inplace=True)\n",
    "        \n",
    "        # Handle outliers with clipping\n",
    "        for col in ndvi_cols:\n",
    "            # NDVI should be between -1 and 1, but source data might have different scaling\n",
    "            q1 = df_processed[col].quantile(0.01)  # 1st percentile\n",
    "            q3 = df_processed[col].quantile(0.99)  # 99th percentile\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "            df_processed[col] = df_processed[col].clip(lower, upper)\n",
    "        \n",
    "        # Rescale NDVI to standard -1 to 1 range if necessary\n",
    "        max_abs_ndvi = max(abs(df_processed[ndvi_cols].min().min()), \n",
    "                          abs(df_processed[ndvi_cols].max().max()))\n",
    "        \n",
    "        # Only normalize if the values seem to be on a different scale\n",
    "        if max_abs_ndvi > 3:  # If values are far outside normal NDVI range\n",
    "            print(\"Rescaling NDVI values to standard range...\")\n",
    "            for col in ndvi_cols:\n",
    "                df_processed[col] = df_processed[col] / max_abs_ndvi\n",
    "        \n",
    "        # Final clip to ensure -1 to 1 range\n",
    "        for col in ndvi_cols:\n",
    "            df_processed[col] = df_processed[col].clip(-1, 1)\n",
    "            \n",
    "        return df_processed\n",
    "    \n",
    "    def extract_features(self, df, ndvi_cols):\n",
    "        \"\"\"Extract features focused on land cover types\"\"\"\n",
    "        print(\"Extracting features...\")\n",
    "        features_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Convert to numpy array for faster processing\n",
    "        ndvi_data = df[ndvi_cols].values\n",
    "        \n",
    "        # Basic statistical features\n",
    "        features_df['ndvi_mean'] = np.mean(ndvi_data, axis=1)\n",
    "        features_df['ndvi_std'] = np.std(ndvi_data, axis=1)\n",
    "        features_df['ndvi_min'] = np.min(ndvi_data, axis=1)\n",
    "        features_df['ndvi_max'] = np.max(ndvi_data, axis=1)\n",
    "        features_df['ndvi_range'] = features_df['ndvi_max'] - features_df['ndvi_min']\n",
    "        features_df['ndvi_median'] = np.median(ndvi_data, axis=1)\n",
    "        \n",
    "        # Vegetation presence counts at different thresholds\n",
    "        # High NDVI values indicate dense vegetation (forest)\n",
    "        features_df['ndvi_above_02_count'] = (ndvi_data > 0.2).sum(axis=1)\n",
    "        features_df['ndvi_above_04_count'] = (ndvi_data > 0.4).sum(axis=1)\n",
    "        features_df['ndvi_above_06_count'] = (ndvi_data > 0.6).sum(axis=1)\n",
    "        \n",
    "        # Low/negative NDVI values indicate water or bare surfaces\n",
    "        features_df['ndvi_below_0_count'] = (ndvi_data < 0).sum(axis=1)\n",
    "        features_df['ndvi_below_02_count'] = (ndvi_data < 0.2).sum(axis=1)\n",
    "        \n",
    "        # Quarter analysis for seasonal patterns\n",
    "        n_points = len(ndvi_cols)\n",
    "        quarter_size = n_points // 4\n",
    "        \n",
    "        for i in range(4):\n",
    "            start_idx = i * quarter_size\n",
    "            end_idx = (i + 1) * quarter_size if i < 3 else n_points\n",
    "            quarter_data = ndvi_data[:, start_idx:end_idx]\n",
    "            \n",
    "            # Quarterly statistics\n",
    "            features_df[f'ndvi_q{i+1}_mean'] = np.mean(quarter_data, axis=1)\n",
    "            features_df[f'ndvi_q{i+1}_max'] = np.max(quarter_data, axis=1)\n",
    "        \n",
    "        # Seasonal change detection (simple but effective)\n",
    "        for i in range(3):  # Compare adjacent quarters\n",
    "            features_df[f'ndvi_q{i+1}_to_q{i+2}_diff'] = (\n",
    "                features_df[f'ndvi_q{i+2}_mean'] - features_df[f'ndvi_q{i+1}_mean']\n",
    "            )\n",
    "        \n",
    "        # Add some raw NDVI values (evenly spaced across time)\n",
    "        indices = np.linspace(0, n_points-1, 5).astype(int)\n",
    "        for i, idx in enumerate(indices):\n",
    "            if idx < len(ndvi_cols):\n",
    "                features_df[f'ndvi_t{i}'] = df[ndvi_cols[idx]]\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        features_df = features_df.fillna(0)\n",
    "        \n",
    "        print(f\"Generated {features_df.shape[1]} features\")\n",
    "        return features_df\n",
    "    \n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        print(\"Starting preprocessing...\")\n",
    "        \n",
    "        # Extract NDVI columns\n",
    "        self.ndvi_columns = self.extract_ndvi_columns(df)\n",
    "        print(f\"Found {len(self.ndvi_columns)} NDVI time points\")\n",
    "        \n",
    "        # Clean and convert data\n",
    "        df_clean = self.clean_data(df, self.ndvi_columns)\n",
    "        \n",
    "        # Preprocess NDVI values\n",
    "        df_processed = self.preprocess_ndvi(df_clean, self.ndvi_columns)\n",
    "        \n",
    "        # Extract features\n",
    "        features_df = self.extract_features(df_processed, self.ndvi_columns)\n",
    "        \n",
    "        # Add ID column if present\n",
    "        if 'ID' in df.columns:\n",
    "            features_df['ID'] = df['ID']\n",
    "        \n",
    "        return features_df, df_processed\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Train the model with careful monitoring\"\"\"\n",
    "        print(\"Training Random Forest model...\")\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y_train)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_scaled, y_encoded)\n",
    "        \n",
    "        # Report out-of-bag score (built-in cross-validation)\n",
    "        print(f\"Out-of-bag score: {self.model.oob_score_:.4f}\")\n",
    "        \n",
    "        # Full evaluation on training data\n",
    "        y_pred = self.model.predict(X_scaled)\n",
    "        print(\"\\nClassification Report (on training data):\")\n",
    "        print(classification_report(y_encoded, y_pred, target_names=self.label_encoder.classes_))\n",
    "        \n",
    "        # Class distribution analysis\n",
    "        pred_class_counts = pd.Series(self.label_encoder.inverse_transform(y_pred)).value_counts()\n",
    "        print(\"\\nPredicted class distribution (training):\")\n",
    "        print(pred_class_counts)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_scaled = self.scaler.transform(X_test)\n",
    "        y_pred_encoded = self.model.predict(X_scaled)\n",
    "        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        X_scaled = self.scaler.transform(X_test)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and prediction pipeline\"\"\"\n",
    "    print(\"NDVI Land Cover Classification Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = NDVILandCoverClassifier(random_state=42)\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        print(\"Loading training data...\")\n",
    "        train_df = pd.read_csv('hacktrain.csv')\n",
    "        print(f\"Training data shape: {train_df.shape}\")\n",
    "        print(f\"Classes: {train_df['class'].value_counts()}\")\n",
    "        \n",
    "        # Preprocess training data\n",
    "        X_train_processed, train_df_clean = classifier.preprocess_data(train_df, is_training=True)\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_columns = [col for col in X_train_processed.columns if col not in ['ID', 'class']]\n",
    "        X_train = X_train_processed[feature_columns]\n",
    "        y_train = train_df['class']\n",
    "        \n",
    "        print(f\"Training features shape: {X_train.shape}\")\n",
    "        \n",
    "        # Train model\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Analyze feature importance\n",
    "        analyze_feature_importance(classifier, feature_columns)\n",
    "        \n",
    "        # Load test data\n",
    "        print(\"\\nLoading test data...\")\n",
    "        test_df = pd.read_csv('hacktest.csv')\n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        \n",
    "        # Preprocess test data\n",
    "        X_test_processed, _ = classifier.preprocess_data(test_df, is_training=False)\n",
    "        X_test = X_test_processed[feature_columns]\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        predictions = classifier.predict(X_test)\n",
    "        \n",
    "        # Prepare submission\n",
    "        submission_df = pd.DataFrame({\n",
    "            'ID': test_df['ID'],\n",
    "            'class': predictions\n",
    "        })\n",
    "        \n",
    "        # Save submission\n",
    "        submission_df.to_csv('submission.csv', index=False)\n",
    "        print(f\"\\nSubmission saved to 'submission.csv'\")\n",
    "        print(f\"Predictions distribution:\")\n",
    "        print(submission_df['class'].value_counts())\n",
    "        \n",
    "        # Display sample predictions\n",
    "        print(f\"\\nSample predictions:\")\n",
    "        print(submission_df.head(10))\n",
    "        \n",
    "        return classifier, submission_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        print(\"Full error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def analyze_feature_importance(classifier, feature_names):\n",
    "    \"\"\"Analyze feature importance from the trained model\"\"\"\n",
    "    if hasattr(classifier.model, 'feature_importances_'):\n",
    "        # Get feature importances\n",
    "        importances = classifier.model.feature_importances_\n",
    "        \n",
    "        # Create feature importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 15 Most Important Features:\")\n",
    "        print(importance_df.head(15))\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Feature importances not available\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier, submission = main()\n",
    "    \n",
    "    if classifier is not None:\n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        print(\"Check your directory for 'submission.csv' file.\")\n",
    "    else:\n",
    "        print(\"\\nPipeline failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c2a9f-4d1e-4ece-a523-6c5d9cee9136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
