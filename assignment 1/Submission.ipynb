{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0b3107-41af-43fb-9359-55bedb782b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI Land Cover Classification\n",
      "==================================================\n",
      "Loading training data...\n",
      "Training set shape: (8000, 30)\n",
      "Class distribution:\n",
      "class\n",
      "forest        6159\n",
      "farm           841\n",
      "impervious     669\n",
      "grass          196\n",
      "water          105\n",
      "orchard         30\n",
      "Name: count, dtype: int64\n",
      "Preprocessing data...\n",
      "Training Random Forest model...\n",
      "OOB Score: 0.8769\n",
      "Training performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        farm       0.65      0.93      0.77       841\n",
      "      forest       0.99      0.92      0.95      6159\n",
      "       grass       0.71      0.97      0.82       196\n",
      "  impervious       0.91      0.91      0.91       669\n",
      "     orchard       0.56      1.00      0.71        30\n",
      "       water       0.94      0.96      0.95       105\n",
      "\n",
      "    accuracy                           0.92      8000\n",
      "   macro avg       0.79      0.95      0.85      8000\n",
      "weighted avg       0.94      0.92      0.93      8000\n",
      "\n",
      "Predicted class distribution:\n",
      "forest        5706\n",
      "farm          1195\n",
      "impervious     669\n",
      "grass          268\n",
      "water          108\n",
      "orchard         54\n",
      "Name: count, dtype: int64\n",
      "Loading test data...\n",
      "Test set shape: (2845, 29)\n",
      "Preprocessing data...\n",
      "Generating predictions...\n",
      "Submission file saved as 'submission.csv'\n",
      "Class distribution in predictions:\n",
      "class\n",
      "forest        1453\n",
      "farm           633\n",
      "impervious     407\n",
      "grass          214\n",
      "water          118\n",
      "orchard         20\n",
      "Name: count, dtype: int64\n",
      "Sample predictions:\n",
      "   ID   class\n",
      "0   1  forest\n",
      "1   2  forest\n",
      "2   3  forest\n",
      "3   4    farm\n",
      "4   5  forest\n",
      "5   6    farm\n",
      "6   7    farm\n",
      "7   8  forest\n",
      "8   9    farm\n",
      "9  10    farm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class NDVILandCoverClassifier:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            oob_score=True,\n",
    "            class_weight='balanced',\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.ndvi_columns = []\n",
    "\n",
    "    def extract_ndvi_columns(self, df):\n",
    "        ndvi_cols = [col for col in df.columns if col.endswith('_N')]\n",
    "        ndvi_cols.sort()\n",
    "        return ndvi_cols\n",
    "\n",
    "    def clean_data(self, df, ndvi_cols):\n",
    "        df_clean = df.copy()\n",
    "        for col in ndvi_cols:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        return df_clean\n",
    "\n",
    "    def preprocess_ndvi(self, df, ndvi_cols):\n",
    "        df_processed = df.copy()\n",
    "\n",
    "        for col in ndvi_cols:\n",
    "            median_val = df_processed[col].median()\n",
    "            if np.isnan(median_val):\n",
    "                median_val = 0.3\n",
    "            df_processed[col].fillna(median_val, inplace=True)\n",
    "\n",
    "        for col in ndvi_cols:\n",
    "            q1 = df_processed[col].quantile(0.01)\n",
    "            q3 = df_processed[col].quantile(0.99)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "            df_processed[col] = df_processed[col].clip(lower, upper)\n",
    "\n",
    "        max_val = max(abs(df_processed[ndvi_cols].min().min()), abs(df_processed[ndvi_cols].max().max()))\n",
    "        if max_val > 3:\n",
    "            for col in ndvi_cols:\n",
    "                df_processed[col] = df_processed[col] / max_val\n",
    "\n",
    "        for col in ndvi_cols:\n",
    "            df_processed[col] = df_processed[col].clip(-1, 1)\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "    def extract_features(self, df, ndvi_cols):\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        ndvi_data = df[ndvi_cols].values\n",
    "\n",
    "        features['mean'] = np.mean(ndvi_data, axis=1)\n",
    "        features['std'] = np.std(ndvi_data, axis=1)\n",
    "        features['min'] = np.min(ndvi_data, axis=1)\n",
    "        features['max'] = np.max(ndvi_data, axis=1)\n",
    "        features['range'] = features['max'] - features['min']\n",
    "        features['median'] = np.median(ndvi_data, axis=1)\n",
    "\n",
    "        features['above_0.2'] = (ndvi_data > 0.2).sum(axis=1)\n",
    "        features['above_0.4'] = (ndvi_data > 0.4).sum(axis=1)\n",
    "        features['above_0.6'] = (ndvi_data > 0.6).sum(axis=1)\n",
    "        features['below_0'] = (ndvi_data < 0).sum(axis=1)\n",
    "        features['below_0.2'] = (ndvi_data < 0.2).sum(axis=1)\n",
    "\n",
    "        total_points = len(ndvi_cols)\n",
    "        quarter = total_points // 4\n",
    "        for i in range(4):\n",
    "            start = i * quarter\n",
    "            end = (i + 1) * quarter if i < 3 else total_points\n",
    "            quarter_data = ndvi_data[:, start:end]\n",
    "            features[f'q{i+1}_mean'] = np.mean(quarter_data, axis=1)\n",
    "            features[f'q{i+1}_max'] = np.max(quarter_data, axis=1)\n",
    "\n",
    "        for i in range(3):\n",
    "            features[f'diff_q{i+1}_to_q{i+2}'] = features[f'q{i+2}_mean'] - features[f'q{i+1}_mean']\n",
    "\n",
    "        selected_points = np.linspace(0, total_points-1, 5).astype(int)\n",
    "        for i, idx in enumerate(selected_points):\n",
    "            features[f'ndvi_t{i}'] = df[ndvi_cols[idx]]\n",
    "\n",
    "        return features.fillna(0)\n",
    "\n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.ndvi_columns = self.extract_ndvi_columns(df)\n",
    "        cleaned = self.clean_data(df, self.ndvi_columns)\n",
    "        processed = self.preprocess_ndvi(cleaned, self.ndvi_columns)\n",
    "        features = self.extract_features(processed, self.ndvi_columns)\n",
    "\n",
    "        if 'ID' in df.columns:\n",
    "            features['ID'] = df['ID']\n",
    "        return features, processed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Training Random Forest model...\")\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y_encoded)\n",
    "\n",
    "        print(f\"OOB Score: {self.model.oob_score_:.4f}\")\n",
    "        y_pred = self.model.predict(X_scaled)\n",
    "        print(\"Training performance:\")\n",
    "        print(classification_report(y_encoded, y_pred, target_names=self.label_encoder.classes_))\n",
    "\n",
    "        print(\"Predicted class distribution:\")\n",
    "        print(pd.Series(self.label_encoder.inverse_transform(y_pred)).value_counts())\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.label_encoder.inverse_transform(self.model.predict(X_scaled))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"NDVI Land Cover Classification\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    classifier = NDVILandCoverClassifier(random_state=42)\n",
    "\n",
    "    try:\n",
    "        print(\"Loading training data...\")\n",
    "        train_df = pd.read_csv(\"hacktrain.csv\")\n",
    "        print(f\"Training set shape: {train_df.shape}\")\n",
    "        print(\"Class distribution:\")\n",
    "        print(train_df['class'].value_counts())\n",
    "\n",
    "        X_train_df, _ = classifier.preprocess_data(train_df)\n",
    "        y_train = train_df['class']\n",
    "        X_train = X_train_df.drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Loading test data...\")\n",
    "        test_df = pd.read_csv(\"hacktest.csv\")\n",
    "        print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "        X_test_df, _ = classifier.preprocess_data(test_df, is_training=False)\n",
    "        X_test = X_test_df.drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "        print(\"Generating predictions...\")\n",
    "        preds = classifier.predict(X_test)\n",
    "\n",
    "        output = pd.DataFrame({'ID': test_df['ID'], 'class': preds})\n",
    "        output.to_csv('submission.csv', index=False)\n",
    "\n",
    "        print(\"Submission file saved as 'submission.csv'\")\n",
    "        print(\"Class distribution in predictions:\")\n",
    "        print(output['class'].value_counts())\n",
    "        print(\"Sample predictions:\")\n",
    "        print(output.head(10))\n",
    "\n",
    "        return classifier, output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during execution:\", e)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def analyze_feature_importance(classifier, features):\n",
    "    if hasattr(classifier.model, 'feature_importances_'):\n",
    "        df = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': classifier.model.feature_importances_\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "        print(\"Top 15 important features:\")\n",
    "        print(df.head(15))\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Feature importance not available.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier, submission = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
